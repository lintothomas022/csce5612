---
title: "Lab 4 Activity Recognition with ML and IMU"
last_modified_at: 2025-05-09
categories:
  - Labs
---
For this lab we collected accelerometer data from Xiao nRF52840 Sense and trained
a machine learning model.

In the beginning of the class we created a flow chart of a activity tracker.
<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/Activity tracker flowchart.jpg" alt="flowchart" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    Activity tracker flowchart
  </figcaption>
</figure>

## **Trial Data Collection**
We use a simple Arduino program to stream accelerometer sensor data from the Seeed Xiao board. Below code was used for it:

```ruby
#include "LSM6DS3.h"
LSM6DS3 myIMU(I2C_MODE, 0x6A);
const float accelerationThreshold = 2; // threshold of significant in G's
const int numSamples = 119;

int samplesRead = numSamples;

void setup() {
    // put your setup code here, to run once:
    Serial.begin(9600);
    while (!Serial);
    //Call .begin() to configure the IMUs
    if (myIMU.begin() != 0) {
    Serial.println("Device error");
    } else {
    Serial.println("Device OK!");
    }
}

void loop() {
  float x, y, z;

  while (samplesRead == numSamples) {
    x=myIMU.readFloatAccelX();
    y=myIMU.readFloatAccelY();
    z=myIMU.readFloatAccelZ();

      float aSum = fabs(x) + fabs(y) + fabs(z);
        // check if it's above the threshold
        if (aSum >= accelerationThreshold) {
        // reset the sample read count
        samplesRead = 0;
        break;
        }
}

while (samplesRead < numSamples) {
    x=myIMU.readFloatAccelX();
    y=myIMU.readFloatAccelY();
    z=myIMU.readFloatAccelZ();

    samplesRead++;

    Serial.print(x, 3);
    Serial.print(',');
    Serial.print(y, 3);
    Serial.print(',');
    Serial.print(z, 3);
    Serial.println();

    if (samplesRead == numSamples) {
    // Serial.println();
    }
  }
}
```

<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/Accessing accelerometer data.png" alt="trial" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    Accessing Accelerometer Data
  </figcaption>
</figure>

## **Capturing Gestures Training Data**
We used CoolTerm software for data collection for gesture recognintion. 
Collected data was transferring to a Text/Binary file and later at the end it is saved csv file by adding "filename" + ".csv".

<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/CoolTerm.png" alt="coolterm" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    Gesture Data Collection
  </figcaption>
</figure>

## **Training Data in TensorFlow**
We trained our data from [Google Colab Project](https://colab.research.google.com/drive/1klo02FTMv_koNJZQi7iLG8aY1bwj9r6V#scrollTo=uvDA8AK7QOq-) and the "model.h" after finishing the training was uploaded to IMU Classifier in arduino IDE.

We achieved low mean absolute error (MAE) and loss scores by training for 40 epochs and using the other hyperparameters shown below.

<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/Hyperparameters.png" alt="Hyperparameters" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    Hyperparameters
  </figcaption>
</figure>

<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/Mean absolute error (left) and loss scores (right) during model training and validaion.png" alt="mae" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    Mean absolute error (left) and loss scores (right) during model training and validaion
  </figcaption>
</figure>

## **Model Testing**
<figure style="text-align: center; width: 100%; max-width: 600px; margin: auto 0 20px auto;">
  <img src="/csce5612/assets/lab_4/IMU Classifier.png" alt="IMU Classifier" style="width: 100%; height: 400px; display: block; margin-bottom: 0;">
  <figcaption style="display: block; text-align: center; font-style: italic; margin-top: 6px; margin: 0 auto; margin-bottom: 20px;">
    IMU Classifier
  </figcaption>
</figure>
